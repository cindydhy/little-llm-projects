{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628462a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c4dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "514eedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='o4-mini-2025-04-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7732e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the DuckDuckGo search tool\n",
    "tools = DuckDuckGoSearchRun(\n",
    "        name=\"Web Search\",\n",
    "        description=\"Useful for searching information on the internet. Use this when you need to find current or factual information.\",\n",
    "        verbose = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c30339be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91b054a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/4fw_96792q73dm48k4zk7_p80000gn/T/ipykernel_6229/167193580.py:34: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the agent\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=PromptTemplate.from_template(template)\n",
    ")\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    \"\"\"\n",
    "    Function to ask a question to the agent and get a response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"input\": query})\n",
    "        # response = tools.invoke(query)\n",
    "        # response = agent_executor.invoke(\"\")\n",
    "        return response.get(\"output\", \"Sorry, I couldn't generate a response.\")\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/Documents/llm/web-search/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe meaning of MANY is consisting of or amounting to a large but indefinite number. How to use many in a sentence. Find 347 different ways to say MANY, along with antonyms, related words, and example sentences at Thesaurus.com. MANY definition: 1. used mainly in negative sentences and questions and with \"too\", \"so\", and \"as\" to mean \"a large…. Learn more. You use many to indicate that you are talking about a large number of people or things. I don't think many people would argue with that. Not many films are made in Finland. Do you keep many … 1. a. a large number of: many coaches; many times. b. (as pronoun; functioning as plural): many are seated already. 2. (foll by: a, an, or another, and a singular noun) each of a considerable number …\u001b[0mAn error occurred: 'str' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "query = \"how many hydrogen fuel stations in the bay area\"\n",
    "print(ask_question(query))\n",
    "\n",
    "# limitations of using duckduckgosearchrun and an agent executor:\n",
    "# whole query doesn't seem to be understood. the model seems sometimes focused on certain keywords at the beginning\n",
    "# results from the search doesn't seem to be coming from a search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac57f79",
   "metadata": {},
   "source": [
    "# Using a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f1581e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_template(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04c11b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = load_prompt_template('prompt.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcbd53f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got ChatPromptTemplate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create PromptTemplate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m react_prompt = \u001b[43mPromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create agent\u001b[39;00m\n\u001b[32m      5\u001b[39m agent = create_react_agent(llm, tools, react_prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llm/web-search/.venv/lib/python3.13/site-packages/langchain_core/prompts/prompt.py:289\u001b[39m, in \u001b[36mPromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, template_format, partial_variables, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_template\u001b[39m(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m     **kwargs: Any,\n\u001b[32m    258\u001b[39m ) -> PromptTemplate:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[32m    260\u001b[39m \n\u001b[32m    261\u001b[39m \u001b[33;03m    *Security warning*:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    287\u001b[39m \u001b[33;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     input_variables = \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     _partial_variables = partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llm/web-search/.venv/lib/python3.13/site-packages/langchain_core/prompts/string.py:255\u001b[39m, in \u001b[36mget_template_variables\u001b[39m\u001b[34m(template, template_format)\u001b[39m\n\u001b[32m    252\u001b[39m     input_variables = _get_jinja2_variables_from_template(template)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m template_format == \u001b[33m\"\u001b[39m\u001b[33mf-string\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    254\u001b[39m     input_variables = {\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     }\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m template_format == \u001b[33m\"\u001b[39m\u001b[33mmustache\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    258\u001b[39m     input_variables = mustache_template_vars(template)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/string.py:288\u001b[39m, in \u001b[36mFormatter.parse\u001b[39m\u001b[34m(self, format_string)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_string\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformatter_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected str, got ChatPromptTemplate"
     ]
    }
   ],
   "source": [
    "# Create PromptTemplate\n",
    "react_prompt = PromptTemplate.from_template(prompt)\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "# Create agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for main query\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question using the most up-to-date information you can find: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc42bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RunnableSequence\n",
    "sequence = RunnableSequence(\n",
    "    prompt,\n",
    "    lambda prompt_result: agent_executor.invoke({\n",
    "        \"input\": prompt_result.to_messages()[0].content,\n",
    "        \"tools\": tools,\n",
    "        \"tools_names\": \", \".join([tool.name for tool in tools]),\n",
    "        \"agent_scratchpad\": \"\"\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/Documents/llm/web-search/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "Mayflower long distance moving customers are serviced by movers at over 500 locations, and we can help ease the burden of moving any distance, long or short. From full service moving to … At Mayflower, we aim to simplify the moving quote process so that you can make the right decision for your move and your budget. We understand that customers want to review … Mayflower’s long-distance movers are experienced tackling long-distance moves of any size. Whether you are moving a small apartment or need to move employees of a corporation to a … Experience stress-free moves with Mayflower® in Colorado Springs, CO. As the nation's trusted moving company since 1927, we offer customizable services crafted to your needs, ensuring a … Mayflower is a trusted full service moving company. Our full service movers will help you no matter where you live. Request a quote today.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Use sequence\n",
    "result = sequence.invoke({\n",
    "    \"question\": \"mayflower dublin restaurant\"\n",
    "})\n",
    "\n",
    "# the answers here are limited to factual info\n",
    "print(\"Answer: \")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
